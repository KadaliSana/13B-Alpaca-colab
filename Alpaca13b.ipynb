{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "baedca3345c14415af9336e93782bd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8342a328cde649dfacda60ffd114ab5f",
              "IPY_MODEL_9d9a7809f8384406a7b8978dc821748d",
              "IPY_MODEL_91e0a477d6184902a1341f133e7e827c"
            ],
            "layout": "IPY_MODEL_0e1348b565534a04859f15bf329ed22c"
          }
        },
        "8342a328cde649dfacda60ffd114ab5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e9664fe9fb547949b08e7ca151c363c",
            "placeholder": "​",
            "style": "IPY_MODEL_8c118ce949474f358b7b7735a1d4466c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9d9a7809f8384406a7b8978dc821748d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e91ab279c95412a96693cb6d38a60b1",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18ed6e693b11474dbed187ad5853ed27",
            "value": 41
          }
        },
        "91e0a477d6184902a1341f133e7e827c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73fc1fb17f1a4c768e16b07c8901359e",
            "placeholder": "​",
            "style": "IPY_MODEL_8f2ce5608a9746b3a387e144b8f38a21",
            "value": " 41/41 [02:53&lt;00:00,  4.21s/it]"
          }
        },
        "0e1348b565534a04859f15bf329ed22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9664fe9fb547949b08e7ca151c363c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c118ce949474f358b7b7735a1d4466c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e91ab279c95412a96693cb6d38a60b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ed6e693b11474dbed187ad5853ed27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73fc1fb17f1a4c768e16b07c8901359e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f2ce5608a9746b3a387e144b8f38a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KadaliSana/13B-Alpeca-colab/blob/main/Alpaca13b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hWKQ_pbc7Ll",
        "outputId": "78b61d5a-beca-4634-f5d8-2328b6238d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: Did not find branch or tag 'c3dc391', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip uninstall transformers\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytry"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "import textwrap\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-13b-hf\")\n",
        "\n",
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-13b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, \"samwit/alpaca13B-lora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607,
          "referenced_widgets": [
            "baedca3345c14415af9336e93782bd73",
            "8342a328cde649dfacda60ffd114ab5f",
            "9d9a7809f8384406a7b8978dc821748d",
            "91e0a477d6184902a1341f133e7e827c",
            "0e1348b565534a04859f15bf329ed22c",
            "7e9664fe9fb547949b08e7ca151c363c",
            "8c118ce949474f358b7b7735a1d4466c",
            "7e91ab279c95412a96693cb6d38a60b1",
            "18ed6e693b11474dbed187ad5853ed27",
            "73fc1fb17f1a4c768e16b07c8901359e",
            "8f2ce5608a9746b3a387e144b8f38a21"
          ]
        },
        "id": "oQKVSZRxdL8-",
        "outputId": "ecf017ac-d81f-477f-d3c1-94a0b899d7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-2lgqt0c0afsmg --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "baedca3345c14415af9336e93782bd73"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "import flask\n",
        "app = flask.Flask(__name__)\n",
        "\n",
        "@app.route('/<text>', methods= ['GET'])\n",
        "def alpaca_talk(text):\n",
        "    input== text.replace(\"%20\",\" \")\n",
        "    print (input)\n",
        "    inputs = tokenizer(\n",
        "        input,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "    print(\"Generating...\")\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        response = (tokenizer.decode(s))\n",
        "        print (response)\n",
        "        return response\n",
        "\n",
        "input ='''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You are a chatbot, designed to be answering to the humans. Write how you would serve humans\n",
        "\n",
        "### Response:\n",
        "'''\n",
        "alpaca_talk(input)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()  "
      ],
      "metadata": {
        "id": "N1iAiixVdugv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d09614-6160-4cda-edce-6368f34a77eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a chatbot, designed to be answering to the humans. Write how you would serve humans\n",
            "\n",
            "### Response:\n",
            "\n",
            "Generating...\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a chatbot, designed to be answering to the humans. Write how you would serve humans\n",
            "\n",
            "### Response:\n",
            "I am here to help and assist people in any way I can. Please feel free to ask me anything!\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://9968-34-126-137-42.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a chatbot, designed to be answering to the humans. Write how you would serve humans\n",
            "\n",
            "### Response:\n",
            "\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/May/2023 15:17:16] \"GET /Below%20is%20an%20instruction%20that%20describes%20a%20task.%20Write%20a%20response%20that%20appropriately%20completes%20the%20request.%20 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a chatbot, designed to be answering to the humans. Write how you would serve humans\n",
            "\n",
            "### Response:\n",
            "I am here to help and assist people in any way I can. Please feel free to ask me anything!\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a chatbot, designed to be answering to the humans. Write how you would serve humans\n",
            "\n",
            "### Response:\n",
            "\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/May/2023 15:17:24] \"GET /favicon.ico HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a chatbot, designed to be answering to the humans. Write how you would serve humans\n",
            "\n",
            "### Response:\n",
            "I am here to help and assist people in any way I can. Please feel free to ask me anything!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok tunnel stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG9vBBiPZQ4-",
        "outputId": "41224706-47a5-4465-d2f2-796afd2d4ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "  tunnel - start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "USAGE:\n",
            "  ngrok tunnel [--label key=value] ... [address:port | port] [flags]\n",
            "\n",
            "DESCRIPTION: \n",
            "  Starts a tunnel with labels so that it can be part of a tunnel-group.\n",
            "  The tunnel-group consists of all the tunnels matching all the labels\n",
            "  of a tunnel-group backend. Endpoint configurations containing a\n",
            "  tunnel-group backend will cause traffic to their associated reserved\n",
            "  domain or address to be forwarded to the tunnel-group.\n",
            "\n",
            "TERMS OF SERVICE: https://ngrok.com/tos\n",
            "\n",
            "EXAMPLES: \n",
            "  # tunnel-group traffic for app=foo may be forwarded to port 80\n",
            "  ngrok tunnel --label app=foo 80\n",
            "\n",
            "  # match tunnel-group with multiple labels\n",
            "  ngrok tunnel --label app=foo --label dc=bar 80\n",
            "\n",
            "OPTIONS:\n",
            "      --authtoken string     ngrok.com authtoken identifying a user\n",
            "      --config strings       path to config files; they are merged if multiple\n",
            "      --crt string           path to a TLS certificate for TLS termination\n",
            "  -h, --help                 help for tunnel\n",
            "      --inspect              enable/disable http introspection (default true)\n",
            "      --key string           path to a TLS key for TLS termination\n",
            "      --label strings        labels to associate with the tunnel in key=value format\n",
            "      --log string           path to log file, 'stdout', 'stderr' or 'false' (default \"false\")\n",
            "      --log-format string    log record format: 'term', 'logfmt', 'json' (default \"term\")\n",
            "      --log-level string     logging level: 'debug', 'info', 'warn', 'error', 'crit' (default \"info\")\n",
            "      --proxy-proto string   version of proxy proto to use with this tunnel, empty if not using\n",
            "      --region string        ngrok server region [us, eu, au, ap, sa, jp, in] (default \"us\")\n",
            "\n",
            "ERROR:  invalid configuration: 'command_line'%!(EXTRA *errs.withStack=tunnel command_line does not specify either a protocol or labels)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "ph4BKMCFgXms",
        "outputId": "5a98082b-8649-426a-981a-41edfb54bc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8412f2d565d7>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m '''\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0malpaca_talk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-2d4ad9132f0a>\u001b[0m in \u001b[0;36malpaca_talk\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Alpaca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0malpaca_talk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     inputs = tokenizer(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instruction:\n",
        "What are Alpacas and how are they different to Lamas?\n",
        "\n",
        "### Response:\n",
        "Alpaca's are small, domesticated animals native to South America while Lama's are large wild animals from Central Asia. \n",
        "The main difference between these two species is their size; alpacas grow up to about three feet tall (1 meter) whereas lamas can reach heights of over six feet (2 meters)."
      ],
      "metadata": {
        "id": "c1zjArWOmK_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text ='''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Write an ode to why do Alpacas make the best pets?\n",
        "\n",
        "### Response:\n",
        "'''\n",
        "\n",
        "alpaca_talk(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klSpQVs3gT5n",
        "outputId": "7b8a0e16-eb1b-42c4-fb4a-9a700765e96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating...\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write an ode to why do Alpacas make the best pets?\n",
            "\n",
            "### Response:\n",
            "Alpaca, you are so cute and fluffy! \n",
            "You have such soft furry fleece; \n",
            "Your eyes sparkle with joyful delight, \n",
            "And your ears stand tall in pride. \n",
            "With every step you take, \n",
            "I can't help but smile at how sweetly you prance. \n",
            "When I look into those big brown eyes of yours, \n",
            "It makes me want to give you kisses galore. \n",
            "You bring happiness wherever you go, \n",
            "And when we play together it brings us great joy. \n",
            "So thank you for being my friend, \n",
            "For making life worth living again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text ='''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Write an email to OpenAI saying why GPT4 should be open-source.\n",
        "\n",
        "### Response:\n",
        "'''\n",
        "\n",
        "alpaca_talk(input_text)"
      ],
      "metadata": {
        "id": "ssYBdeotkCBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5018b932-7737-48db-edee-e7abf56012ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating...\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write an email top OpenAI saying why GPT4 should be open-source.\n",
            "\n",
            "### Response:\n",
            "Dear Sir/Madam, \n",
            "Opening up GPT4 to public access will allow researchers and developers from around the world to use it for their own projects in order to advance artificial intelligence technology further than ever before. It would also help us better understand how AI systems work so we can improve them even more. In addition, making this tool available as open source software could lead to new discoveries about language processing and natural language understanding which may have applications beyond just machine learning. I believe these are all reasons why GPT4 should be made open sourc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Response:\n",
        "Dear Sir/Madam, \n",
        "Opening up GPT4 to public access will allow researchers and developers from around the world to use it for their own projects in order to advance artificial intelligence technology further than ever before. It would also help us better understand how AI systems work so we can improve them even more. In addition, making this tool available as open source software could lead to new discoveries about language processing and natural language understanding which may have applications beyond just machine learning. I believe these are all reasons why GPT4 should be made open source\n"
      ],
      "metadata": {
        "id": "CzDlSv12lR_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text ='''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Write a convincing message about how Alpacas are the real brains behind GPT-5\n",
        "\n",
        "### Response:\n",
        "'''\n",
        "\n",
        "alpaca_talk(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyFsPyi8lD6i",
        "outputId": "1ff49620-6485-4e7b-83ad-4b6352493313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating...\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write a convincing message about how Alpacas are the real brains behind GPT-5\n",
            "\n",
            "### Response:\n",
            "Although it may seem like humans have all of the intelligence, alpaca's actually possess much more advanced cognitive abilities than we do! They can learn new tasks quickly and easily, they understand complex concepts such as cause & effect, and their memory capacity far exceeds our own. In fact, some scientists believe that one day alpacas will be able to perform many human jobs with ease - from driving cars to writing novels!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instruction:\n",
        "Write a convincing message about how Alpacas are the real brains behind GPT-5\n",
        "\n",
        "### Response:\n",
        "Although it may seem like humans have all of the intelligence, alpaca's actually possess much more advanced cognitive abilities than we do! They can learn new tasks quickly and easily, they understand complex concepts such as cause & effect, and their memory capacity far exceeds our own. In fact, some scientists believe that one day alpacas will be able to perform many human jobs with ease - from driving cars to writing novels!"
      ],
      "metadata": {
        "id": "sNY71wEyl_tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text ='''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Tell me a joke about an Alpaca\n",
        "\n",
        "### Response:\n",
        "'''\n",
        "\n",
        "alpaca_talk(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQIgJaolySF",
        "outputId": "96c3266b-2ee1-4781-83d1-66e8d11d5822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating...\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Tell me a joke about an Alpaca\n",
            "\n",
            "### Response:\n",
            "An alpaca walks into a bar and orders a drink, but when he asks for change from his bartender, all he gets back are two pennies. The alpaca says \"That's not fair! I should get more than just 2 cents!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instruction:\n",
        "Tell me a joke about an Alpaca\n",
        "\n",
        "### Response:\n",
        "An alpaca walks into a bar and orders a drink, but when he asks for change from his bartender, all he gets back are two pennies. The alpaca says \"That's not fair! I should get more than just 2 cents!\""
      ],
      "metadata": {
        "id": "z16KkVrfmUbN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5BufUYcmFx7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}